{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "L5ALSPQSJp-n"
   },
   "source": [
    "# GD with MNIST_revision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGD, MBGD, SGD의 비교\n",
    "\n",
    "- BGD의 장점\n",
    "    - 전체 데이터에 대해 업데이트가 한번에 이루어지기 때문에 후술할 SGD 보다 업데이트 횟수가 적다. 따라서 전체적인 계산 횟수는 적다.\n",
    "    - 전체 데이터에 대해 error gradient 를 계산하기 때문에 optimal 로의 수렴이 안정적으로 진행된다.\n",
    "    - 병렬 처리에 유리하다.\n",
    "- BGD의 단점\n",
    "    - 한 스텝에 모든 학습 데이터 셋을 사용하므로 학습이 오래 걸린다.\n",
    "    - 전체 학습 데이터에 대한 error 를 모델의 업데이트가 이루어지기 전까지 축적해야 하므로 더 많은 메모리가 필요하다.\n",
    "    - local optimal 상태가 되면 빠져나오기 힘듦(SGD 에서 설명하겠음.)\n",
    "- SGD의 장점\n",
    "    - 위 그림에서 보이듯이 Shooting 이 일어나기 때문에 local optimal 에 빠질 리스크가 적다.\n",
    "    - step 에 걸리는 시간이 짧기 때문에 수렴속도가 상대적으로 빠르다.\n",
    "- SGD의 단점\n",
    "    - global optimal 을 찾지 못 할 가능성이 있다.\n",
    "    - 데이터를 한개씩 처리하기 때문에 GPU의 성능을 전부 활용할 수 없다.\n",
    "- MBGD의 장점\n",
    "    - BGD보다 local optimal 에 빠질 리스크가 적다.\n",
    "    - SGD보다 병렬처리에 유리하다.\n",
    "    - 전체 학습데이터가 아닌 일부분의 학습데이터만 사용하기 때문에 메모리 사용이 BGD 보다 적다.\n",
    "- MBGD의 단점\n",
    "    - batch size(mini-batch size) 를 설정해야 한다.\n",
    "    - 에러에 대한 정보를 mini-batch 크기 만큼 축적해서 계산해야 하기 때문에 SGD 보다 메모리 사용이 높다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.nomidl.com/wp-content/uploads/2022/08/image-11.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "\n",
    "z = np.arange(-5, 5, 0.1)\n",
    "phi = 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.plot(z, phi)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('phi')\n",
    "plt.title('Sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function definition\n",
    "- In linear regression, loss is defined as \n",
    "$ \\displaystyle \\mathcal{L}(w, b) = \\frac{1}{2n}\\sum_{i=0}^{N-1}(\\hat{y_i}-y_i)^2 = \\frac{1}{2n}\\sum_{i=0}^{N-1}\\{(wx_i+b)-y_i\\}^2 $.\n",
    "- To calculate derivatives, we use $ \\displaystyle \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial z} \\frac{\\partial z}{\\partial w} = \\frac{1}{n}((wx_i+b)-y_i)x_i$ for sample $i$.\n",
    "- For classification problem, how do we define loss function?\n",
    "\n",
    "### Logistic loss function ($y$, $\\hat{y}$ is 0 or 1)\n",
    "- $ \\displaystyle \\mathcal{L} = -\\{y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})\\} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss definition according to phi (Logistic loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 150\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "x = np.arange(0.001, 1-0.001, 0.001)\n",
    "cost_0 = -1 * np.log(1 - x)\n",
    "cost_1 = -1 * np.log(x)\n",
    "\n",
    "axs[0].plot(x, cost_0)\n",
    "axs[1].plot(x, cost_1)\n",
    "axs[0].set_xlabel('y_hat')\n",
    "axs[1].set_xlabel('y_hat')\n",
    "fig.supylabel('Loss')\n",
    "axs[0].set_title(\"if y = 0\")\n",
    "axs[1].set_title(\"if y = 1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous GD code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_accuracy(weights, X, Y):\n",
    "    correct = 0\n",
    "    for samplenum in range(X.shape[0]):\n",
    "        input = X[samplenum].reshape(WIDTH*HEIGHT, 1)\n",
    "        pred = np.dot(weights, input)\n",
    "        pred = np.argmax(pred)\n",
    "        if pred == Y[samplenum]:\n",
    "            correct += 1\n",
    "    return correct / X.shape[0]\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255\n",
    "WIDTH = X_train.shape[1]\n",
    "HEIGHT = X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0],WIDTH*HEIGHT) / SCALE_FACTOR\n",
    "X_test = X_test.reshape(X_test.shape[0],WIDTH*HEIGHT)  / SCALE_FACTOR\n",
    "\n",
    "weights = np.random.rand(10, WIDTH*HEIGHT) - 0.5\n",
    "alpha = 0.01\n",
    "iteration = 100000\n",
    "\n",
    "for iter in range(iteration):\n",
    "    ind = np.random.randint(X_train.shape[0])\n",
    "    input = X_train[ind].reshape(WIDTH*HEIGHT, 1)\n",
    "    true = Y_train[ind]\n",
    "    true = np.array([1 if i == true else 0 for i in range(10)]).reshape(10, 1)\n",
    "    \n",
    "    pred = np.dot(weights, input)\n",
    "    error = (pred - true) ** 2\n",
    "    delta = pred - true\n",
    "\n",
    "    weight_deltas = np.outer(delta, input)\n",
    "    weights -= alpha * weight_deltas\n",
    "\n",
    "    if iter % np.round(iteration/10) == 0:\n",
    "        print(f\"Iteration: {iter}, \\\n",
    "              Accuracy for train: {check_accuracy(weights, X_train, Y_train)}\")\n",
    "\n",
    "print(f\"Accuracy for test: {check_accuracy(weights, X_test, Y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "``` python\n",
    "pred_1 = [[0.9]\n",
    "          [0.8]\n",
    "          [0.7]],\n",
    "pred_2 = [[0.5]\n",
    "          [0.2]\n",
    "          [0.1]]\n",
    "```\n",
    "Which prediction is better?\n",
    "\n",
    "- Softmax function --> Output intensity normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss function\n",
    "- $ \\displaystyle \\mathcal{L} = -\\{y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})\\} $.\n",
    "### Cross-entropy loss function (Generalized version of logistic loss function)\n",
    "- $ \\displaystyle \\mathcal{L} = -\\sum_{c=1}^C y_c \\log(\\hat{y}) = -\\log(\\hat{y}_{y=1})$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved code with softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_accuracy(weights, X, Y):\n",
    "    correct = 0\n",
    "    for samplenum in range(X.shape[0]):\n",
    "        input = X[samplenum].reshape(WIDTH*HEIGHT, 1)\n",
    "        pred = np.dot(weights, input)\n",
    "        pred = np.argmax(pred)\n",
    "        if pred == Y[samplenum]:\n",
    "            correct += 1\n",
    "    return correct / X.shape[0]\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "SCALE_FACTOR = 255\n",
    "WIDTH = X_train.shape[1]\n",
    "HEIGHT = X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0],WIDTH*HEIGHT) / SCALE_FACTOR\n",
    "X_test = X_test.reshape(X_test.shape[0],WIDTH*HEIGHT)  / SCALE_FACTOR\n",
    "\n",
    "weights = np.random.rand(10, WIDTH*HEIGHT) - 0.5\n",
    "alpha = 0.01\n",
    "iteration = 100000\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.clip(x, -100, 100)   # overflow 방지\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def cross_entropy(pred, true):\n",
    "    return -np.sum(true * np.log(pred))\n",
    "\n",
    "for iter in range(iteration):\n",
    "    ind = np.random.randint(X_train.shape[0])\n",
    "    input = X_train[ind].reshape(WIDTH*HEIGHT, 1)\n",
    "    true = Y_train[ind]\n",
    "    true = np.array([1 if i == true else 0 for i in range(10)]).reshape(10, 1)\n",
    "    \n",
    "    pred = np.dot(weights, input)\n",
    "    pred = softmax(pred)    # softmax를 적용해준다.\n",
    "    error = cross_entropy(pred, true)   # cross entropy를 적용해준다.\n",
    "    delta = pred - true # 사실 이 부분은 굉장히 복잡한데, 여기서는 간단하게 생각하자.\n",
    "\n",
    "    weight_deltas = np.outer(delta, input)\n",
    "    weights -= alpha * weight_deltas\n",
    "\n",
    "    if iter % np.round(iteration/10) == 0:\n",
    "        print(f\"Iteration: {iter}, \\\n",
    "              Accuracy for train: {check_accuracy(weights, X_train, Y_train)}\")\n",
    "\n",
    "print(f\"Accuracy for test: {check_accuracy(weights, X_test, Y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important!\n",
    "1. 로지스틱 회귀에서 SGD로 진행되는 논리 숙지할 것.\n",
    "- 1이라고 확신했는데 (즉, z 값이 아주 높았었는데,) 사실 0이었다면?\n",
    "    - Loss가 아주 높아야 함\n",
    "- 1이라고 확신했는데 (즉, z 값이 아주 높았었는데,) 역시 1이었다면?\n",
    "    - Loss가 아주 낮아야 함\n",
    "2. BGD, MBGD, SGD의 이해\n",
    "3. Softmax function, cross-entropy loss function의 이해"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4-2 확률적 경사 하강법.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
